import json
import requests
import time
import streamlit as st
import pandas as pd
from .db_utils import Database
import os

# ‡∏™‡∏£‡πâ‡∏≤‡∏á global database instance
db = Database()

def init_database():
    """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ"""
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏£‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if 'db_initialized' in st.session_state:
        return True

    print("\nüîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")

    has_sqlite = os.path.exists('data/database.sqlite')
    json_files = {
        'datasets': 'data/datasets_info.json',
        'resources': 'data/dataset_files.json'
    }
    has_json = all(os.path.exists(path) for path in json_files.values())

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô SQLite ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if has_sqlite:
        try:
            cursor = db.conn.execute("SELECT COUNT(*) FROM datasets")
            count = cursor.fetchone()[0]
            if count > 0:
                print(f"‚úÖ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SQLite ‡πÅ‡∏•‡πâ‡∏ß ({count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
                st.session_state.db_initialized = True
                return True
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}")
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡πÉ‡∏´‡πâ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå database ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
            try:
                os.remove('data/database.sqlite')
                print("üîÑ ‡∏•‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà")
                has_sqlite = False
            except:
                pass

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå JSON ‡πÉ‡∏´‡πâ migrate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô
    if has_json:
        print("\nüì• ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå JSON ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ migrate...")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô JSON
        try:
            with open(json_files['datasets'], 'r', encoding='utf-8') as f:
                datasets = json.load(f)
            with open(json_files['resources'], 'r', encoding='utf-8') as f:
                resources = json.load(f)
            print(f"üìä ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(datasets)} ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞ {len(resources)} ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£")
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON: {str(e)}")
            datasets = []
            resources = []

        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô JSON ‡πÉ‡∏´‡πâ migrate
        if len(datasets) > 0 and len(resources) > 0:
            print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á migrate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
            if db.migrate_from_json():
                print("‚úÖ Migrate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                st.session_state.db_initialized = True
                return True
            else:
                print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ migrate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ")
        else:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á SQLite ‡πÅ‡∏•‡∏∞ JSON
    if not has_sqlite and not has_json:
        print("\nüîÑ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        sample_data = {
            'datasets': [{
                'package_id': 'sample',
                'title': '‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•',
                'organization': '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á',
                'url': 'https://data.go.th',
                'resource_count': 1,
                'file_types': 'CSV',
                'last_updated': '2024-03-19'
            }],
            'resources': [{
                'dataset_id': 'sample',
                'file_name': 'sample.csv',
                'format': 'CSV',
                'url': 'https://data.go.th',
                'description': '‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á',
                'ranking': 4
            }]
        }
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á database
        if db.init_sample_data(sample_data):
            print("‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            st.session_state.db_initialized = True
            return True
        else:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ")
            return False

    return False

def load_datasets():
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    if not init_database():
        st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ")
        return None
    
    try:
        data = db.get_datasets()
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        df = pd.DataFrame(data)
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        print("Columns in DataFrame:", df.columns.tolist())
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        print("\nSample data:")
        print(df.head())
        
        # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô SQL ‡πÅ‡∏•‡πâ‡∏ß
        return df
    except Exception as e:
        print(f"Error loading datasets: {str(e)}")
        st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: {str(e)}")
        return None

def update_dataset(package_id):
    """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• dataset"""
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á progress bar
        progress_text = "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..."
        progress_bar = st.progress(0, text=progress_text)
        
        print(f"\nüîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dataset ID: {package_id}")
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API
        progress_bar.progress(0.2, text="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API...")
        PACKAGE_SHOW_URL = f"https://data.go.th/api/3/action/package_show?id={package_id}"
        response = requests.get(PACKAGE_SHOW_URL, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if not data.get("success"):
            progress_bar.empty()
            print("‚ùå API ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ")
            return "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÑ‡∏î‡πâ"
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó
        progress_bar.progress(0.4, text="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
        package = data['result']
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• dataset
        resources = package.get('resources', [])
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞ URL
        file_types = set()
        resource_urls = []
        for resource in resources:
            if resource.get('format'):
                file_types.add(resource['format'].upper())
            if resource.get('url'):
                resource_urls.append(resource['url'])
        
        dataset_data = {
            'package_id': package_id,
            'title': package.get('title', ''),
            'organization': package.get('organization', {}).get('title', ''),
            'url': package.get('url', '') or (resource_urls[0] if resource_urls else ''),
            'last_updated': package.get('metadata_modified', ''),
            'resource_count': len(resources),
            'file_types': ', '.join(sorted(file_types)) if file_types else ''
        }
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• resources
        resources_data = []
        for resource in resources:
            file_format = resource.get('format', '').upper()
            resources_data.append({
                'dataset_id': package_id,
                'file_name': resource.get('name', '') or f"‡πÑ‡∏ü‡∏•‡πå {file_format}" if file_format else '‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠',
                'format': file_format,
                'url': resource.get('url', ''),
                'description': resource.get('description', ''),
                'ranking': 0  # ‡πÄ‡∏Å‡πá‡∏ö ranking ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ
            })
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        progress_bar.progress(0.8, text="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
        
        # ‡∏î‡∏∂‡∏á ranking ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó
        existing_resources = db.get_dataset_files(package_id)
        ranking_map = {r['file_name']: r.get('ranking', 0) for r in existing_resources} if existing_resources else {}
        
        # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó ranking ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        for resource in resources_data:
            resource['ranking'] = ranking_map.get(resource['file_name'], 0)
        
        if db.update_dataset(dataset_data, resources_data):
            # ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
            progress_bar.progress(1.0, text="‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
            time.sleep(1)
            progress_bar.empty()
            
            print(f"‚úÖ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {package_id}")
            print(f"üìÅ ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå: {dataset_data['file_types']}")
            print(f"üîó URL: {dataset_data['url']}\n")
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó session state
            if 'last_update' not in st.session_state:
                st.session_state.last_update = {}
            st.session_state.last_update[package_id] = time.time()
            
            return "‚úÖ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"
        else:
            progress_bar.empty()
            return "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ"
    except Exception as e:
        if 'progress_bar' in locals():
            progress_bar.empty()
        error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"
        print(f"‚ùå {error_msg}")
        return f"‚ùå {error_msg}"

def update_dataset_ranking(dataset_id, ranking):
    """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó ranking ‡∏Ç‡∏≠‡∏á dataset"""
    return db.update_dataset_ranking(dataset_id, ranking)

def get_dataset_files(package_id):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á dataset ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏"""
    db = Database()
    return db.get_dataset_files(package_id)

def get_dataset_rankings(package_ids):
    """‡∏î‡∏∂‡∏á ranking ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ datasets ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô"""
    db = Database()
    return db.get_dataset_rankings(package_ids) 